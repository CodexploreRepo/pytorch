{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Using a Variational Autoencoder\n",
    "- Anomaly detection is one of the most widespread use cases for unsupervised machine learning, especially in industrial applications.\n",
    "- Applications of anomaly detection: fraud detection in banking, preventive maintenance in heavy industries, threat detection in cybersecurity\n",
    "- Challenge: defining outliers explicitly\n",
    "- Variational autoencoders (VAEs): \n",
    "    - The `encoder` is trained to learn the general structure of the training data to isolate only its discriminative features, which are summarised in a compact **latent vector**. \n",
    "    - The `latent vector` constitutes an information bottleneck that *forces* the model to be very *selective about what to encode*. \n",
    "    - A `decoder` is to re-construct the original data from the latent vector as faithfully as possible. \n",
    "    - Outlier detection: when presented with an out-of-distribution sample (an outlier), the system will not be able to make an accurate reconstruction. \n",
    "        - By detecting in-accurate reconstructions, we can tell which examples are outliers.\n",
    "- Reference:\n",
    "    - [Kaggle Notebook](https://www.kaggle.com/code/lucfrachon/anomaly-detection-using-vaes)\n",
    "    - [Anomaly Detection Using a Variational Autoencoder](https://medium.com/@luc.frachon/anomaly-detection-using-a-variational-autoencoder-part-i-e48cd26c027d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview\n",
    "- Goal: tracking the internal temperature of some sizeable industrial machine about which we have no prior knowledege about (like what kind of machine or industry) and detect if any abnomaly happens.\n",
    "- Dataset: a one-dimensional time series &#8594; engineer features and make the data multi-dimensional to make things more interesting and relevant.\n",
    "- Approach:\n",
    "    - Gather and preprocess data, including train/test split,\n",
    "    - Build a VAE and train it on the training set,\n",
    "    - Pass test samples to the VAE and record the reconstruction loss for each,\n",
    "    - Identify test samples with a high reconstruction loss and flag them as anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "- There is a challenge in feature engineering as not having much information about what kind of machine or industry, we will therefore have to make assumptions.\n",
    "- Assumptions (A):\n",
    "    - A1: The timestamps cover the Christmas and New Year holidays. Since we are dealing with an industrial machine, it stands to reason that its workload might be affected by holidays and maybe even by the proximity (in time) to a holiday (`gap_holiday`). \n",
    "        - A1.1: applicable holidays are those typical in Europe and the Americas\n",
    "    - A2: There is the difference in temparature between \n",
    "        - A2.1: every hour in the day \n",
    "        - A2.2: weekdays and weekends\n",
    "        - A2.3: day in the month\n",
    "        - A2.4: month to month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "- Categorical features (`day_of_week`, `holiday`): encode `day_of_week` from 0 to 6, and `holiday` as 0 or 1.\n",
    "- Continous features (`gap_holiday`, `t`, `value`): is essential to normalise the continuous variables as the weights of a neural network are randomly initialised from a shared distribution, so they all tend to have similar scales initially. \n",
    "    - Test data must be normalised using statistics observed on the train set to avoid leaking information from the test set into the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "- Time series data: we take the last 30% of the observations (in chronological order) as our test set\n",
    "    - Train set: 15,900 timestamps\n",
    "    - Test set: 6,800 timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- The encoder's first layer: a concatenation of the continuous variables and embedding vectors encoding the categorical variables.\n",
    "    - Embedding vectors (learnable vectors of dimension 16 in our experiments) are heavily used in Natural Language Processing and, more generally, when working with discrete data. \n",
    "        - For example: the vector `day_of_week` can take seven values (one for each day), each 16-dimensional. After training, the vector for day_of_week==0 might, for example, capture the fact that the machine's activity is lower on Sundays. \n",
    "- Hyper-parameters of fully connected layers: (`layer_dims`) and neurons per layer `(32, 64, 128, 256)`\n",
    "    - Each layer can be batch-normalised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a layer as a sequence of {fully-connected unit, batch normalisation, leaky-ReLU activation}\n",
    "class Layer(nn.Module):\n",
    "    '''\n",
    "    A single fully connected layer with optional batch \n",
    "    normalisation and activation.\n",
    "    '''\n",
    "    def __init__(self, in_dim, out_dim, bn = True):\n",
    "        super().__init__()\n",
    "        layers = [nn.Linear(in_dim, out_dim)]\n",
    "        if bn: layers.append(nn.BatchNorm1d(out_dim))\n",
    "        layers.append(nn.LeakyReLU(0.1, inplace=True))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
