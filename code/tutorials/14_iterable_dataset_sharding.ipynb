{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IterableDataset — Sharding with num_workers and pin_memory\n",
    "\n",
    "Demonstrates:\n",
    "- How to write an `IterableDataset` with correct sharding\n",
    "- Why `num_workers > 0` without sharding causes duplicate data\n",
    "- How `pin_memory=True` + `non_blocking=True` fits in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Dataset\n",
    "\n",
    "Simulates a streaming dataset (e.g. rows from a CSV or log file).\n",
    "Each sample is a `(features, label)` pair generated from a simple linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Simulates a streaming dataset with 'total_samples' rows.\n",
    "    Each row: features = random float tensor, label = sum of features.\n",
    "    \"\"\"\n",
    "    def __init__(self, total_samples: int, n_features: int = 4, seed: int = 42):\n",
    "        super().__init__()\n",
    "        self.total_samples = total_samples\n",
    "        self.n_features = n_features\n",
    "        self.seed = seed\n",
    "\n",
    "        # Pre-generate all data once (simulates a fixed file on disk)\n",
    "        rng = torch.Generator().manual_seed(seed)\n",
    "        self.features = torch.randn(total_samples, n_features, generator=rng)\n",
    "        self.labels   = self.features.sum(dim=1, keepdim=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "\n",
    "        if worker_info is None:\n",
    "            # ── Single-process DataLoader (num_workers=0) ──────────────────\n",
    "            # No sharding needed: yield every sample\n",
    "            start, end = 0, self.total_samples\n",
    "        else:\n",
    "            # ── Multi-process DataLoader (num_workers > 0) ─────────────────\n",
    "            # Divide samples into contiguous blocks, one block per worker\n",
    "            #\n",
    "            # Example: 8 samples, 2 workers\n",
    "            #   worker 0 → indices 0..3\n",
    "            #   worker 1 → indices 4..7\n",
    "            total    = self.total_samples\n",
    "            n_workers = worker_info.num_workers\n",
    "            wid       = worker_info.id\n",
    "\n",
    "            # Base chunk size (floor division)\n",
    "            chunk = total // n_workers\n",
    "            # Distribute leftover samples to first (total % n_workers) workers\n",
    "            remainder = total % n_workers\n",
    "\n",
    "            # Workers with id < remainder get one extra sample\n",
    "            if wid < remainder:\n",
    "                start = wid * (chunk + 1)\n",
    "                end   = start + chunk + 1\n",
    "            else:\n",
    "                start = wid * chunk + remainder\n",
    "                end   = start + chunk\n",
    "\n",
    "        for idx in range(start, end):\n",
    "            yield self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Sharding is Correct\n",
    "\n",
    "Check: union of all worker shards == full dataset, with no overlaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_SAMPLES = 20\n",
    "N_FEATURES    = 4\n",
    "BATCH_SIZE    = 4\n",
    "NUM_WORKERS   = 3   # intentionally not a divisor of 20 to test remainder logic\n",
    "\n",
    "dataset = StreamingDataset(total_samples=TOTAL_SAMPLES, n_features=N_FEATURES)\n",
    "\n",
    "# Use num_workers > 0 with pin_memory\n",
    "# Note: pin_memory=True has no effect on MPS/CPU but is safe to set\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,         # pins tensors in CPU RAM for faster GPU transfer\n",
    ")\n",
    "\n",
    "all_labels = []\n",
    "for features, labels in loader:\n",
    "    all_labels.extend(labels.squeeze().tolist())\n",
    "\n",
    "print(f\"Total samples seen : {len(all_labels)}\")\n",
    "print(f\"Unique samples seen: {len(set(round(x, 4) for x in all_labels))}\")\n",
    "print()\n",
    "if len(all_labels) == TOTAL_SAMPLES:\n",
    "    print(\"Sharding OK — every sample seen exactly once\")\n",
    "else:\n",
    "    print(f\"Problem — expected {TOTAL_SAMPLES}, got {len(all_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Show What Goes Wrong Without Sharding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrokenStreamingDataset(IterableDataset):\n",
    "    \"\"\"Same dataset but __iter__ ignores worker_info — no sharding.\"\"\"\n",
    "    def __init__(self, total_samples: int, n_features: int = 4, seed: int = 42):\n",
    "        super().__init__()\n",
    "        self.total_samples = total_samples\n",
    "        rng = torch.Generator().manual_seed(seed)\n",
    "        self.features = torch.randn(total_samples, n_features, generator=rng)\n",
    "        self.labels   = self.features.sum(dim=1, keepdim=True)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # No get_worker_info() check — every worker streams the full dataset\n",
    "        for idx in range(self.total_samples):\n",
    "            yield self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "broken_loader = DataLoader(\n",
    "    BrokenStreamingDataset(total_samples=TOTAL_SAMPLES),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "broken_labels = []\n",
    "for features, labels in broken_loader:\n",
    "    broken_labels.extend(labels.squeeze().tolist())\n",
    "\n",
    "print(f\"Total samples seen : {len(broken_labels)}\")\n",
    "print(f\"Unique samples seen: {len(set(round(x, 4) for x in broken_labels))}\")\n",
    "print()\n",
    "print(f\"Expected {TOTAL_SAMPLES}, got {len(broken_labels)} — \"\n",
    "      f\"{len(broken_labels) // TOTAL_SAMPLES}x duplication\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Full Training Loop\n",
    "\n",
    "Shows the complete pattern: sharded `IterableDataset` + `pin_memory` + `non_blocking` transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# ── Device ────────────────────────────────────────────────────────────────────\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ── Model ─────────────────────────────────────────────────────────────────────\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(N_FEATURES, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 1)\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# ── DataLoader (sharded IterableDataset + pin_memory) ─────────────────────────\n",
    "train_dataset = StreamingDataset(total_samples=1000, n_features=N_FEATURES)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,           # pins CPU tensors for fast DMA transfer to GPU\n",
    "    persistent_workers=True,   # keeps worker processes alive between epochs\n",
    ")\n",
    "\n",
    "# ── Training loop ─────────────────────────────────────────────────────────────\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_batches  = 0\n",
    "\n",
    "    for features, labels in train_loader:\n",
    "        # non_blocking=True pairs with pin_memory=True:\n",
    "        # transfer runs on a separate CUDA stream, CPU continues preparing next batch\n",
    "        features = features.to(device, non_blocking=True)\n",
    "        labels   = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(features)\n",
    "        loss  = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n_batches  += 1\n",
    "\n",
    "    avg_loss = total_loss / n_batches\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | loss={avg_loss:.4f} | lr={current_lr:.2e} | batches={n_batches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "```\n",
    "IterableDataset __iter__\n",
    "  └── get_worker_info()          # None → single process, else → shard\n",
    "        ├── worker_info.id       # which worker am I? (0, 1, 2, ...)\n",
    "        └── worker_info.num_workers\n",
    "\n",
    "DataLoader\n",
    "  ├── num_workers=2              # 2 subprocesses load data in parallel\n",
    "  ├── pin_memory=True            # lock CPU tensors in RAM for fast DMA to GPU\n",
    "  └── persistent_workers=True   # keep workers alive between epochs (avoids respawn cost)\n",
    "\n",
    "Training loop\n",
    "  └── tensor.to(device, non_blocking=True)   # async transfer on separate CUDA stream\n",
    "```\n",
    "\n",
    "### Shard correctness rule\n",
    "```\n",
    "Union of all shards    = full dataset   (no gaps)\n",
    "Intersection of shards = empty          (no overlaps)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
